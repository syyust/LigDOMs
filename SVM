import numpy as np
import pandas as pd
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load data from CSV file
data = pd.read_csv('test.csv')
X = data.drop('lignin', axis=1)  # Features
y = data['lignin']  # Target values

# Feature scaling (SVM is sensitive to feature scale, standardization improves performance)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=60)

# Initialize SVR model
svr_model = SVR()

# Define hyperparameter grid for tuning
param_grid = {
    'kernel': ['linear', 'rbf', 'poly'],  # Kernel function types
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'epsilon': [0.1, 0.2, 0.3],  # Epsilon parameter for epsilon-SVR
    'gamma': ['scale', 'auto', 0.1, 1]  # Gamma parameter for RBF kernel
}

# Use GridSearchCV for hyperparameter optimization with cross-validation
grid_search = GridSearchCV(estimator=svr_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Output best parameters
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score (negative MSE):", -grid_search.best_score_)

# Create model with best parameters
best_svr_model = SVR(**grid_search.best_params_)

from sklearn.model_selection import cross_validate

# Define evaluation metrics
scoring = ['neg_mean_squared_error', 'r2', 'neg_mean_absolute_error']

# Perform cross-validation
cv_results = cross_validate(best_svr_model, X_train, y_train, cv=5, scoring=scoring, return_train_score=True)

# Print cross-validation results
print("Training set MSE:", -cv_results['train_neg_mean_squared_error'])
print("Test set MSE:", -cv_results['test_neg_mean_squared_error'])
print("Training set R²:", cv_results['train_r2'])
print("Test set R²:", cv_results['test_r2'])
print("Training set MAE:", -cv_results['train_neg_mean_absolute_error'])
print("Test set MAE:", -cv_results['test_neg_mean_absolute_error'])

# Train the model
best_svr_model.fit(X_train, y_train)

# Predict on training set
y_train_pred = best_svr_model.predict(X_train)

# Calculate evaluation metrics for training set
mse_train = mean_squared_error(y_train, y_train_pred)
mae_train = mean_absolute_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)

print(f'Training set Mean Squared Error (MSE): {mse_train}')
print(f'Training set Mean Absolute Error (MAE): {mae_train}')
print(f'Training set R² score: {r2_train}')

# Predict on test set
y_test_pred = best_svr_model.predict(X_test)

# Calculate evaluation metrics for test set
mse_test = mean_squared_error(y_test, y_test_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Test set Mean Squared Error (MSE): {mse_test}')
print(f'Test set Mean Absolute Error (MAE): {mae_test}')
print(f'Test set R² score: {r2_test}')

# Plot learning curve to analyze model performance
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Generate learning curve data
train_sizes, train_scores, test_scores = learning_curve(
    best_svr_model, X_train, y_train, cv=5, scoring='r2', train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate mean and standard deviation for training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot learning curve
plt.figure()
plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training R2")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="Test R2")
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.title("Learning Curve")
plt.xlabel("Training examples")
plt.ylabel("R2 Score")
plt.legend(loc="best", fontsize="large")  # Increase legend font size
plt.grid()
plt.show()

# Global prediction on new data
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load new input data for prediction
new_data_file = 'global_predictclay_new.csv'  # New data file path
new_data = pd.read_csv(new_data_file)

# Ensure new data has same features as training data
X_new = new_data.drop('lignin', axis=1, errors='ignore')  # Ignore if 'lignin' column doesn't exist in new data

# Standardize new data using the scaler fitted on training data
X_new_scaled = scaler.fit_transform(X_new)

# Make predictions using trained model
y_new_pred = best_svr_model.predict(X_new_scaled)

# Add predictions to new data DataFrame
new_data['predicted_lignin'] = y_new_pred

# Save predictions to new CSV file
output_file = 'SVM_rf.csv'
new_data.to_csv(output_file, index=False)

print(f"Prediction results saved to file: {output_file}")
print(f"Prediction results saved to file: {output_file}")
